{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding\n",
    "\n",
    "Updated: 2020-07-02\n",
    "\n",
    "This notebook provides a brief example of fMRI decoding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional setup for Google Colab\n",
    "!pip install numpy==1.17\n",
    "!pip install scipy==1.1\n",
    "!pip install hdf5storage\n",
    "!pip install bdpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Google Colaboratory\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#data_dir = '/content/drive/My Drive/path/to/data/directory'\n",
    "\n",
    "# Setup for local run\n",
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stimulus',\n",
       " 'obsoleted',\n",
       " 'sub-04_task-ImageNetTraining_bold_preproc_native_VC_24runs.h5',\n",
       " 'sub-04_task-ImageNetTraining_bold_preproc_native_VC_10cat.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import bdpy\n",
    "import bdpy.ml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.svm\n",
    "import sklearn.metrics\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fMRI data\n",
    "\n",
    "The fMRI data used in this notebook is colected in [Shen, Horikawa, Majima & Kamitani (2019) Deep image reconstruction from human brain activity. PLOS Comput Biol](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633).\n",
    "\n",
    "In brief, the subject was presented natural images selected from ImageNet during fMRI scans.\n",
    "1200 images from 150 object categories (synsets) were used as the stimuli (thus, 8 images/category).\n",
    "50 images in different categories were presented in each run (with 5 one-back repetition trials).\n",
    "Thus, it took 3 runs to cover the 150 categories, and 24 runs to present all the images.\n",
    "In the original study, each image was presented for five times.\n",
    "So it took 120 runs in total.\n",
    "The entier data contains 6000 samples (1200 images x 5 presentations).\n",
    "Please see the paper for more details.\n",
    "\n",
    "In this notebook, we use the subset of the orignal data: fMRI data for stimuli from 10 categories are used.\n",
    "Thus, the example data contains 400 samples (8 images/categories x 10 categories x 5 presentations).\n",
    "\n",
    "The file `sub-04_task-ImageNetTraining_bold_preproc_native_VC_10cat.h5` containes preprocessed fMRI data for 24 runs from one subject.\n",
    "After slice timing correction, motion correction, and coregistration, the fMRI samples were shifted forward for 4 s to compensate hemodynamic delay. Then, head motion parameters, linear-trend, and run-wise mean were regressed out from the fMRI signal. Extreme values in the signal were then removed. Finally, the signal in a single image presentation trial (4 s) were averaged across time. Thus, each sample in the example data correspond to a single image presentation trial.\n",
    "\n",
    "The example fMRI data includes only voxels in the visual cortex (VC; V1, V2, V3, V4, LOC, FFA, PPA).\n",
    "\n",
    "The full dataset is available at <https://openneuro.org/datasets/ds001506/> (raw data) or <https://figshare.com/articles/Deep_Image_Reconstruction/7033577> (preprocessed data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aoki/.local/share/virtualenvs/education-materials-fRAHoN22/lib/python3.7/site-packages/bdpy/bdata/bdata.py:855: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  dat = h5py.File(load_filename)\n",
      "/Users/aoki/.local/share/virtualenvs/education-materials-fRAHoN22/lib/python3.7/site-packages/bdpy/bdata/bdata.py:874: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  if isinstance(v.value, np.ndarray):\n",
      "/Users/aoki/.local/share/virtualenvs/education-materials-fRAHoN22/lib/python3.7/site-packages/bdpy/bdata/bdata.py:875: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  v = [self.__to_unicode(x) for x in v.value]\n",
      "/Users/aoki/.local/share/virtualenvs/education-materials-fRAHoN22/lib/python3.7/site-packages/bdpy/bdata/bdata.py:877: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  v = self.__to_unicode(v.value)\n",
      "/Users/aoki/.local/share/virtualenvs/education-materials-fRAHoN22/lib/python3.7/site-packages/bdpy/bdata/bdata.py:884: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  vmap.update({float(k): self.__to_unicode(dat['vmap'][mk][k].value)})\n"
     ]
    }
   ],
   "source": [
    "bdata = bdpy.BData(os.path.join(data_dir, 'sub-04_task-ImageNetTraining_bold_preproc_native_VC_10cat.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use fMRI responses in the lateral occipital cortex (LOC), which underlies object recongition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI data (samples x voxels): (400, 3066)\n"
     ]
    }
   ],
   "source": [
    "# fMRI data in the lateral occipital cortex (LOC)\n",
    "fmri_data_loc = bdata.select('ROI_LOC')\n",
    "print('fMRI data (samples x voxels): {}'.format(fmri_data_loc.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulus labels\n",
    "stimulus_labels = bdata.get_label('stimulus_name')\n",
    "#stimulus_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run numbers\n",
    "runs = bdata.select('Run')\n",
    "\n",
    "# Regroup runs for cross-validation\n",
    "runs_groups = (runs + 2) // 3\n",
    "#runs_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of object categories\n",
    "\n",
    "Here we try decoding of object categories from the fMRI data.\n",
    "We use linear SVM for the classifier and evaluate prediction performance with leave-three-run-out cross-validation since the three runs cover the all 150 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert stimulus labels (e.g., `n01639765_47681`) to cateogry labels (e.g., `n01639765`).\n",
    "The liberal before `_` in the stimulus labels is ImageNet synset ID representing the category.\n",
    "\n",
    "http://www.image-net.org/synset?wnid=n01639765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert stimulus labels to category labels\n",
    "category_labels = np.array([\n",
    "    lb.split('_')[0]\n",
    "    for lb in stimulus_labels\n",
    "])\n",
    "len(np.unique(category_labels))  # This should be 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything you need for the decoding analysis is ready.\n",
    "The fMRI data is saved as an array of sample-by-feature (voxels), so you can run the decoding with typical machine learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [03:11,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton accuracy: 0.41500000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cvindex = bdpy.ml.cvindex_groupwise(runs_groups)\n",
    "\n",
    "prediction_accuracy_cv = []\n",
    "\n",
    "for ind_train, ind_test in tqdm.tqdm(cvindex):\n",
    "    x_train = fmri_data_loc[ind_train, :]\n",
    "    y_train = category_labels[ind_train]\n",
    "    x_test = fmri_data_loc[ind_test, :]\n",
    "    y_test = category_labels[ind_test]\n",
    "    \n",
    "    # Normalization\n",
    "    norm_mean = np.mean(x_train, axis=0)\n",
    "    norm_scale = np.std(x_train, axis=0, ddof=1)\n",
    "    \n",
    "    x_train = (x_train - norm_mean) / norm_scale\n",
    "    x_test = (x_test - norm_mean) / norm_scale\n",
    "\n",
    "    # Model training\n",
    "    model = sklearn.svm.LinearSVC()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    prediction_accuracy_cv.append(acc)\n",
    "    \n",
    "prediction_accuracy = np.mean(prediction_accuracy_cv)\n",
    "print('Prediciton accuracy: {}'.format(prediction_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the classification with fMRI data in V1 (`ROI_V1`).\n",
    "\n",
    "[Optional] Compare the decoding accuracy between LOC and V1 and discuss the decoding accuracy from the two brain regions are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code comes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer comes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try another classification methods such as logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code comes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Typically, decoding or machine learning-based analysis of fMRI data is suffered from overfitting due to high dimensionality of the features (voxels). Think of a method to solve the overfitting on fMRI, implement it, and see whether it works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code comes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
